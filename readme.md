# Introduction
This repository contains in the terraform folder a Terraform code which creates:
- AKS
- Storage Account - Which will serve as a MLflow artifact store
- ACR - For storing Docker images used when deploying MLflow resources on AKS
- Service Principal - Used for authentication when interacting with ACR and AKS
- Dockerfile - Which can be used to run a container with prepared environment to interact with created AKS. In that container we are gonna have:
    - Installed Helm - Which will be used for installing a MySQL db on AKS which will serve as a MLflow backend store
    - Installed and configured kubectl - Which will be used to interact with AKS
    - Saved YAML files - For deploying MLflow resources on AKS
    - Saved a bash script for deploying MLflow resources on AKS (which uses saved YAML files)
    - Saved a bash script for building and pushing to ACR a Docker image which we will be using for deploying MLflow resources on AKS

Once we run the Terraform code, we will move the generated Dockerfile to the 'docker' folder and use it to run a container.

From that container we will deploy MLflow Tracking Server on AKS. 

Further in this document we have the following main sections:
- Repository guide - How to use this code
- Prerequisites - What we need to do before we start using code from this repo
- Creating Azure resources with Terraform - What Azure resources we create using Terraform and how
- Deploying the MLflow Tracking Server - How do we deploy the MLflow Tracking Server
- Generated Dockerfile - More details about how the Dockerfile generated by Terraform works

**Note:**\
Instead of running a Docker container on our machine, we can prepare environment for interacting with AKS on our host machine itself.

If we have a Linux or MacOS, then we can take bash scripts from this Dockerfile and execute them on our machine.





# Repository guide
Here is described how to use code from this repository.

## Satisfying prerequisites
Before we start using this code we need to satisfy prerequisites described in the 'Prerequisites' section further in this document.

## Creating Azure resources and generating a Dockerfile
We need to create a few resources in Azure:
- AKS
- Storage Account
- ACR

Also we need to have a Dockerfile which we will be used to run a container with prepared environment to interact with AKS.

In order to create all those resources and generate a Dockerfile we need to run the following commands in the terraform folder:
>- terraform init # only when running Terraform for the first time in this repository
>- terraform plan -out main.tfplan
>- terraform apply main.tfplan

## Move generated Dockerfile
The Dockerfile generated by Terraform will be saved in the terraform folder. We need to move it to the docker folder.

## Run a Docker container
Now we need to build a Docker image and run a container using the generated Dockerfile. We can do that using the following commands (run them in the 'docker' folder):
>docker build -t aks .
>docker run -it aks /bin/bash

The `-it` option gives us access to the container's bash session.

All the next steps where we interact with AKS we will be performing from inside of this container.

## Build and push to ACR the MLflow image
Now from inside of the container which we started in the previous step, we can build and push to ACR a Docker image which will be used for deploying MLflow resources on AKS.

We do that by running the mlflow_docker/tracking_server_build_push.sh script.

## Create AKS resources
Now from inside of the container we can run the k8s/mlflow_deploy.sh script which will deploy the following AKS resources:
- Namespace
- MySQL with volume (used as backend store for MLflow Tracking Server)
- Secret with:
    - Azure Storage Account access key
    - MySQL URI (with password, used for accessing this db by MLflow)
- Another secret used for authentication when pulling images from ACR
- MLflow Tracking Server with a Service

## Test MLflow Tracking Server
We can create a Pod which will try to save logs in the backend store and save a file in the artifact store. We can do that by running the below command from inside of the container:
>kubectl apply -f k8s/mlflow_test.yaml

And then check its logs:
>kubectl -n mlflow logs mlflow-test

We should see there messages `Logged a test run` and `Artifact logged`.

## Destroying Azure resources
Once we are done, then in order to destroy all the created resources in Azure we need to run the following commands:
>- terraform plan -destroy -out main.destroy.tfplan
>- terraform apply main.destroy.tfplan








# Prerequisites
## Terraform variables
Before using this code we need to create terraform.tfvars file which look like terraform-draft.tfvars file in the same location. It is described there what values to provide. We are assigning there values to variables from the variables.tf file located in the same folder. In the variables.tf we can also find descriptions of those variables. We need to assign values only for those variables which doesn't have assigned the default value.

## Terraform configuration
We need to configure properly Terraform on our computer so it can create resources in our Azure subscription, it is described here: [developer.hashicorp.com](https://developer.hashicorp.com/terraform/tutorials/azure-get-started/azure-build).

## Azure subscription
We need to have a subscription on the Azure platform portal.azure.com.






# Creating Azure resources with Terraform
This section explains what Azure resources and files we create using Terraform and how.

We have there the main.tf file which creates all the resources. That file uses modules defined in the terraform_linux_vm > modules folder. Each module is dedicated to creating one type of resource in Azure.

Resources which we create are:
- Two resource groups
- Log Analytics workspace - For AKS monitoring.
- AKS
- ACR - For storing Docker images used when deploying resources on AKS.
- Service Principal - With proper scopes and roles which will be used in the Dockerfile generated by Terraform for authentication when interacting with AKS and ACR. More details about assigned roles and scopes can be found in comments in the main.tf file when creating this Service Principal.
- Storage Account - Which will act as an artifact store for MLflow Tracking Server.
- Dockerfile - It gets saved on our local computer and it will be used to create an image with prepared environment to interact with AKS.

## Terraform outputs
Terraform creates multiple outputs with information about created resources. They are not needed to use code from this repo but might be useful in some situations. They can be accessed by running the command:
>terraform output

## Kubeconfig file
It is possible to use Terraform to save on our local machine a kubeconfig file when creating AKS but we don't do this since we generate a kubeconfig file in a Docker image using Azure CLI.






# Deploying the MLflow Tracking Server
This section explains how do we deploy the MLflow Tracking server.

We can divide the process of deploying the MLflow Tracking Server into two parts:
- MLflow Tracking Server - Initial preparations
- Deploy the MLflow Tracking Server as a Deployment

We will also deploy a Service for the Tracking Server's Deployment.

Below are more details about this process. All the steps described here are performed by the Dockerfile generated by Terraform. More details about how all those steps are implemented in this Docker file and further in this document in the 'Generated Dockerfile' section.

## MLflow Tracking Server - Initial preparations
Before we can deploy the MLflow Tracking Server's Deployment we need to perform the following steps:
- Create a Docker Image for MLflow Tracking Server
- Create the 'mlflow' namespace - For all the resources related to the MLflow
- Prepare a MySQL as the Tracking Server's backend store:
    - Prepare a Volume and Volume Claim
    - Install MySQL chart using Helm (and use for it the Volume and Volume Claim created previously)
- Create a secret which will be used by the Tracking Server to connect to the backend and artifact store. It will contain:
    - Azure Storage Account connection string (used for authentication when connecting to the Storage Account which serves as an artifact store)
    - MySQL URI (including a password. This MySQL will serve as a backend store)
- Create a secret for ACR - It will be used for authentication when pulling a Docker Image from ACR when deploying the Tracking Server Deployment

## Deploy the MLflow Tracking Server as a Deployment
After those preparations we can deploy the Tracking Server Deployment. Here are the most important aspects of its specification:
- Deploy in the MLflow namespace
- It is deployed by running the `mlflow server` command in a Pod with proper arguments
- Use a secret for authentication when pulling a Docker Image
- Use the Docker Image we created and pushed to the ACR earlier
- Create environment variables in a Pod with values from the secret for:
    - Azure Storage Account connection string
    - MySQL URI
    
    They will be used by the MLflow Tracking Server for authentication to the artifact store and backend store. 

After that we create a Service for the MLflow Tracking Server Deployment. 






# Generated Dockerfile
Here is described a Docker image created by the Dockerfile generated by the Terraform code.

## Overview
In this image we are going to have:
- Installed Helm - Which will be used for installing a MySQL db on AKS which will serve as a MLflow backend store.
- Installed Azure CLI - Which will be used to get credentials to AKS (create the .kube/config file) and to push to ACR images used when deploying MLflow resources on AKS.
- Installed and configured kubectl - Which will be used to interact with AKS. We will have prepared the .kube/config file used for authentication to AKS which enables us using kubectl.
- Saved a bash script for building and pushing to ACR Docker images which we will be using for deploying MLflow resources on AKS.
- Saved YAML files - For deploying MLflow resources on AKS:
    - Namespace - Where we will deploy all the resources related to MLflow
    - Volume claim - Which will be used by MySQL backend store
    - MLflow Tracking Server
    - Service for the Tracking Server
    - Pod testing the Tracking Server - It will run a simple Python script connecting to the backend and artifact store.
- Saved a bash script for deploying MLflow resources on AKS. That script performs the following actions:
    - Deploys all the resources defined by YAML files listed above (except for the testing Pod)
    - Deploys MySQL using Helm - Which will act as a MLflow backend store
    - Creates a secret with values for connecting to the MySQL (backend store) and Azure Storage Account (artifact store)
    - Creates a secret for authentication to ACR - It is used when pulling images from it when deploying Kubernetes resources

## User
In this image we create a new user with username specified by the Terraform variable 'vm_username' and with password 'admin'.

## Configuring kubectl
We run the `az aks get-credentials` command to create the kubeconfig file (.kube/config). That will allow us to use kubectl to interact with our AKS cluster.

In order to run that command successfully we need to login to Azure using `az login` using a Service Principal with the 'Azure Kubernetes Service Cluster User Role' role. We are using a Service Principal which we created using Terraform.

## Script for building and pushing a Docker image to ACR
In this Dockerfile we are creating the tracking_server_build_push.sh script for building and pushing to ACR a Docker image which will be later on used when deploying MLflow resources on AKS.

In that script we are using the `az acr build` Azure CLI command. In order to run that command successfully we need to login to Azure using `az login` using a Service Principal with the 'Contributor' role. We are using a Service Principal which we created using Terraform.





# AKS resources deployment - details
Here is more details about resources we are deploying on AKS.

## MySQL volume
When creating a volume for MySQL resource we specify `storageClassName: managed-csi` which indicates that we want to use an Azure Disk for storage (this disk gets created when we create a volume).

When we delete that volume then Azure Disk gets deleted as well.